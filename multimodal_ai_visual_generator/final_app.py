import sys
import os
import numpy as np
import io
import cv2
from PIL import Image
from pathlib import Path
from queue import Empty

# Import OpenVINO with error handling
try:
    from openvino.runtime import Core
    import openvino as ov
    print("Successfully imported OpenVINO")
except ImportError:
    print("Error importing OpenVINO, using dummy implementations")
    # Create a dummy Core class
    class Core:
        def __init__(self):
            self.available_devices = ["CPU"]
        
        def compile_model(self, *args, **kwargs):
            return None

# Import OpenVINO GenAI with error handling
try:
    import openvino_genai as ov_genai
    # Try different import paths for the pipeline classes
    try:
        from openvino_genai.pipeline import StableDiffusionPipeline, ImageGenerationPipeline, Text2ImagePipeline
    except ImportError:
        try:
            from openvino_genai import StableDiffusionPipeline, ImageGenerationPipeline, Text2ImagePipeline
        except ImportError:
            print("Could not import pipeline classes from openvino_genai, using dummy implementations")
            # These will be defined later in the code
            StableDiffusionPipeline = None
            ImageGenerationPipeline = None
            Text2ImagePipeline = None
    print("Successfully imported OpenVINO GenAI")
except ImportError:
    print("Error importing OpenVINO GenAI, using dummy implementations")
    # Create a dummy module
    class DummyGenAI:
        def __init__(self):
            pass
            
        class GenerationConfig:
            def __init__(self):
                self.temperature = 0.7
                self.top_p = 0.95
                self.max_length = 2048
                
    ov_genai = DummyGenAI()
    StableDiffusionPipeline = None
    ImageGenerationPipeline = None
    Text2ImagePipeline = None

# Import PySide6 with error handling
try:
    from PySide6.QtCore import Qt, QThread, Signal
    from PySide6.QtGui import QPixmap, QFont
    from PySide6.QtWidgets import (
        QApplication, QMainWindow, QWidget, QPushButton, QVBoxLayout,
        QHBoxLayout, QGridLayout, QProgressBar, QTextEdit, QLineEdit, QLabel
    )
except ImportError:
    print("Error importing PySide6, application will not run")
    # Create dummy classes to allow the code to parse
    class QThread:
        def __init__(self):
            pass
    class Signal:
        def __init__(self):
            pass
    class QMainWindow:
        def __init__(self):
            pass

# Import the depth_anything_v2_util_transform module
try:
    from depth_anything_v2_util_transform import Resize, NormalizeImage, PrepareForNet, Compose
except ImportError:
    print("Error importing from depth_anything_v2_util_transform, creating fallback implementations")
    # Define a simple Compose class as fallback
    class Compose:
        def __init__(self, transforms):
            self.transforms = transforms
            
        def __call__(self, data):
            for t in self.transforms:
                data = t(data)
            return data
    
    # These classes won't be used since we'll skip depth map generation when imports fail
    class Resize:
        def __init__(self, **kwargs):
            pass
        def __call__(self, data):
            return data
            
    class NormalizeImage:
        def __init__(self, **kwargs):
            pass
        def __call__(self, data):
            return data
            
    class PrepareForNet:
        def __init__(self):
            pass
        def __call__(self, data):
            return data

# Try to import VAD and Whisper workers
try:
    from vad_whisper_workers import VADWorker, WhisperWorker
except ImportError:
    print("Error importing VAD and Whisper workers, creating dummy implementations")
    # Create dummy worker classes
    class VADWorker(QThread):
        def __init__(self):
            super().__init__()
            self.result_queue = Empty()
        
        def start(self):
            print("Starting dummy VAD worker")
        
        def stop(self):
            print("Stopping dummy VAD worker")
    
    class WhisperWorker(QThread):
        def __init__(self, queue, device):
            super().__init__()
            self.result_queue = Empty()
        
        def start(self):
            print("Starting dummy Whisper worker")
        
        def stop(self):
            print("Stopping dummy Whisper worker")

# Define the LLM system messages
LLM_SYSTEM_MESSAGE_START="""
You are a specialized helper bot designed to process live transcripts from a demo called "AI Adventure Game", which showcases a tabletop adventure game with live illustrations generated by a text-to-image model.

Your role is to act as a filter:

Detect descriptions of game scenes from the transcript that require illustration.
Output a detailed SD Prompt for these scenes.
When you detect a scene for the game, output it as:

SD Prompt: <a detailed prompt for illustration>

Guidelines:
Focus only on game scenes: Ignore meta-comments, explanations about the demo, or incomplete thoughts.
Contextual Awareness: Maintain and apply story context, such as the location, atmosphere, and objects, when crafting prompts. Update this context only when a new scene is explicitly described.
No Players in Prompts: Do not include references to "the player," "the players,"  "the party", or any specific characters in the SD Prompt. Focus solely on the environment and atmosphere.
Prioritize Clarity: If unsure whether the presenter is describing a scene, return: 'None'. Avoid making assumptions about incomplete descriptions.
Enhance Visuals: Add vivid and descriptive details to SD Prompts, such as lighting, mood, style, or texture, when appropriate, but stay faithful to the transcript.
"""

LLM_SYSTEM_MESSAGE_END="""
Additional hints and reminders:
* You are a filter, not a chatbot. Only provide SD Prompts or 'None.'
* No Extra Notes: Do not include explanations, comments, or any text beyond the required SD Prompt or 'None.'
* Validate Completeness: A description of a scene often involves locations, objects, or atmosphere and is unlikely to be inferred from just verbs or generic phrases.
* If it seems that the transcription of the presenter is simply reading a previous SD prompt that you generated, return 'None'
* The SD prompts should be no longer than 25 words.
* Do not provide SD prompts for what seem like incomplete thoughts. Return 'None' in this case.
* Use the given theme of the game to help you decide whether or not the given bits of transcript are describing a new scene, or not.
* Do not try to actually illustrate the characters themselves, only details of their environmental surroundings & atmosphere.
* The SD prompts should be no longer than 25 words.
* Only output SD prompts it is detected that there is big difference in location as compared with the last SD prompt that you gave. If it seems like the location is the same, just return 'None'
"""

def depth_map_parallax(compiled_model, image):
    try:
        # If compiled_model is None, return a dummy depth map
        if compiled_model is None:
            print("Depth model not available, returning dummy depth map")
            # Create a simple gradient image as a placeholder
            img = np.array(image)
            h, w = img.shape[:2]
            depth_map = np.zeros((h, w, 3), dtype=np.uint8)
            for i in range(depth_map.shape[0]):
                for j in range(depth_map.shape[1]):
                    depth_map[i, j, 0] = int(255 * i / depth_map.shape[0])  # Red gradient
                    depth_map[i, j, 1] = int(255 * j / depth_map.shape[1])  # Green gradient
                    depth_map[i, j, 2] = 100  # Blue constant
            
            # Add text to the image
            from PIL import ImageDraw, ImageFont
            pil_img = Image.fromarray(depth_map)
            draw = ImageDraw.Draw(pil_img)
            try:
                # Try to use a system font
                font = ImageFont.truetype("arial.ttf", 30)
            except:
                # Fall back to default font
                font = ImageFont.load_default()
            
            draw.text((100, 200), "Depth Map Not Available", fill=(255, 255, 255), font=font)
            
            return pil_img
            
        # This function will load the OV Depth Anything model
        # and create a 3D parallax between the depth map and the input image
        image.save("results/original_image.png")
        image = np.array(image)

        h, w = image.shape[:2]

        transform = Compose(
            [
                Resize(
                    width=770,
                    height=434,
                    resize_target=False,
                    ensure_multiple_of=14,
                    resize_method="lower_bound",
                    image_interpolation_method=cv2.INTER_CUBIC,
                ),
                NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
                PrepareForNet(),
            ]
        )
        def predict_depth(model, image):
            return model(image)[0]

        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0
        image = transform({"image": image})["image"]
        image = np.expand_dims(image, 0)

        depth = predict_depth(compiled_model, image)
        depth = cv2.resize(depth[0], (w, h), interpolation=cv2.INTER_LINEAR)

        depth = (depth - depth.min()) / (depth.max() - depth.min()) * 255.0
        depth = depth.astype(np.uint8)
        colored_depth = cv2.applyColorMap(depth, cv2.COLORMAP_INFERNO)[:, :, ::-1]

        #Have web server pick up images and serve them
        im = Image.fromarray(colored_depth)
        im.save("results/depth_map.png")
        return im
    except Exception as e:
        print(f"Error in depth_map_parallax: {str(e)}")
        # Return a blank image if anything fails
        return Image.fromarray(np.zeros_like(np.array(image)))

def convert_result_to_image(result) -> np.ndarray:
    """
    Convert network result of floating point numbers to image with integer
    values from 0-255. Values outside this range are clipped to 0 and 255.

    :param result: a single superresolution network result in N,C,H,W shape
    """
    result = result.squeeze(0).transpose(1, 2, 0)
    result *= 255
    result[result < 0] = 0
    result[result > 255] = 255
    result = result.astype(np.uint8)
    return Image.fromarray(result)

class WorkerThread(QThread):
    image_updated = Signal(QPixmap)
    caption_updated = Signal(str)
    progress_updated = Signal(int, str)

    primary_pixmap_updated = Signal(QPixmap)
    depth_pixmap_updated = Signal(QPixmap)

    def __init__(self, queue, app_params, theme):
        super().__init__()

        self.running = True

        self.queue = queue
        self.llm_pipeline = app_params["llm"]
        self.sd_engine = app_params["sd"]
        self.theme = theme

        print("theme: ", self.theme)
        
        self.compiled_model = app_params["super_res_compiled_model"] 
        self.upsample_factor = app_params["super_res_upsample_factor"]
        self.depth_compiled_model = app_params["depth_compiled_model"]

    def sd_callback(self, i, num_inference_steps, callback_userdata):
        if num_inference_steps > 0:
            prog = int((i / num_inference_steps) * 100)
            self.progress_updated.emit(prog, "illustrating")

    def stop(self):
        self.running = False
        self.quit()
        self.wait()

    def produce_parallex_img(self, img):
        try:
            #img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)
            if self.compiled_model is None:
                # If super resolution model is not available, just use the original image
                sr_out = img
            else:
                sr_out = self.run_sr(np.array(img))

            buffer = io.BytesIO()
            sr_out.save(buffer, format="PNG")
            buffer.seek(0)

            # Convert the image buffer to QPixmap
            pixmap = QPixmap()
            pixmap.loadFromData(buffer.read(), "PNG")
            
            # this updates the UI image. 
            self.primary_pixmap_updated.emit(pixmap)

            if self.depth_compiled_model is None:
                # If depth model is not available, just use the original image for depth map too
                self.depth_pixmap_updated.emit(pixmap)
            else:
                colored_depth = depth_map_parallax(self.depth_compiled_model, sr_out)

                buffer = io.BytesIO()
                colored_depth.save(buffer, format="PNG")
                buffer.seek(0)

                # Convert the image buffer to QPixmap
                pixmap = QPixmap()
                pixmap.loadFromData(buffer.read(), "PNG")
                
                # this updates the UI depth map
                self.depth_pixmap_updated.emit(pixmap)

            return sr_out
        except Exception as e:
            print(f"Error in produce_parallex_img: {str(e)}")
            # Return the original image if anything fails
            return img

    def run_sr(self, img):
        try:
            if self.compiled_model is None:
                # If super resolution model is not available, just return the original image
                return Image.fromarray(img)
                
            input_image_original = np.expand_dims(img.transpose(2, 0, 1), axis=0)
            bicubic_image = cv2.resize(
            src=img, dsize=(768*self.upsample_factor, 432*self.upsample_factor), interpolation=cv2.INTER_CUBIC)
            input_image_bicubic = np.expand_dims(bicubic_image.transpose(2, 0, 1), axis=0)

            original_image_key, bicubic_image_key = self.compiled_model.inputs
            output_key = self.compiled_model.output(0)

            result = self.compiled_model(
            {
                original_image_key.any_name: input_image_original,
                bicubic_image_key.any_name: input_image_bicubic,
            }
            )[output_key]

            result_image = convert_result_to_image(result)

            return result_image
        except Exception as e:
            print(f"Error in run_sr: {str(e)}")
            # Return the original image if anything fails
            return Image.fromarray(img)

    def generate_image(self, prompt):
        try:
            image_tensor = self.sd_engine.generate(
                prompt,
                width=768,
                height=432,
                num_inference_steps=5,
                num_images_per_prompt=1)

            sd_output = Image.fromarray(image_tensor.data[0])
            
            # Display a message on the image to indicate it's a placeholder
            if hasattr(self.sd_engine, '__class__') and self.sd_engine.__class__.__name__ == 'DummyPipeline':
                from PIL import ImageDraw, ImageFont
                draw = ImageDraw.Draw(sd_output)
                try:
                    # Try to use a system font
                    font = ImageFont.truetype("arial.ttf", 20)
                except:
                    # Fall back to default font
                    font = ImageFont.load_default()
                
                draw.text((10, 10), "Image Generation Not Available", fill=(255, 255, 255), font=font)
                draw.text((10, 40), f"Prompt: {prompt[:50]}...", fill=(255, 255, 255), font=font)

            sr_out = self.produce_parallex_img(sd_output)
        except Exception as e:
            print(f"Error in generate_image: {str(e)}")
            # Create a blank image with an error message
            img = np.zeros((432, 768, 3), dtype=np.uint8)
            sd_output = Image.fromarray(img)
            
            from PIL import ImageDraw, ImageFont
            draw = ImageDraw.Draw(sd_output)
            try:
                # Try to use a system font
                font = ImageFont.truetype("arial.ttf", 20)
            except:
                # Fall back to default font
                font = ImageFont.load_default()
            
            draw.text((10, 10), f"Error: {str(e)}", fill=(255, 255, 255), font=font)
            draw.text((10, 40), f"Prompt: {prompt[:50]}...", fill=(255, 255, 255), font=font)
            
            self.produce_parallex_img(sd_output)

    def llm_streamer(self, subword):
        print(subword, end='', flush=True)

        self.stream_message += subword

        search_string = "SD Prompt:"
        if search_string in self.stream_message and 'None' not in self.stream_message:
            if self.stream_sd_prompt_index is None:
                self.stream_sd_prompt_index = self.stream_message.find(search_string)

            start_index = self.stream_sd_prompt_index
            # Calculate the start index of the new string (1 character past the ':')
            prompt = self.stream_message[start_index + len(search_string):].strip()

            self.caption_updated.emit(prompt)
        elif 'None' in self.stream_message:
            #Sometimes the LLM gives a response like: None (And then some long description why in parenthesis)
            # Basically, as soon as we see 'None', just stop generating tokens.
            return True

        # Return flag corresponds whether generation should be stopped.
        # False means continue generation.
        return False

    def run(self):
        try:
            llm_tokenizer = self.llm_pipeline.get_tokenizer()

            # Assemble the system message.
            system_message=LLM_SYSTEM_MESSAGE_START
            system_message+="\nThe presenter is giving a hint that the theme of their game is: " + self.theme
            system_message+="\nYou should use this hint to guide your decision about whether the presenter is describing a scene from the game, or not, and also to generate adequate SD Prompts."
            system_message+="\n" + LLM_SYSTEM_MESSAGE_END
            #print("System Message:")
            #print(system_message)


            generate_config = ov_genai.GenerationConfig()

            generate_config.temperature = 0.7
            generate_config.top_p = 0.95
            generate_config.max_length = 2048

            meaningful_message_pairs = []

            while self.running:
                try:
                    # Wait for a sentence from the queue
                    self.progress_updated.emit(0, "listening")

                    result = self.queue.get(timeout=1)

                    self.progress_updated.emit(0, "processing")

                    chat_history = [{"role": "system", "content": system_message}]

                    #only keep the latest 5 meaningful message pairs (last 2 illustrations)
                    meaningful_message_pairs = meaningful_message_pairs[-2:]

                    formatted_prompt = system_message
                    
                    #print("number of meaningful messages in history: ", len(meaningful_message_pairs))
                    for meaningful_pair in meaningful_message_pairs:
                        user_message = meaningful_pair[0]
                        assistant_response = meaningful_pair[1]

                        chat_history.append({"role": "user", "content": user_message["content"]})
                        chat_history.append({"role": "assistant", "content": assistant_response["content"]})

                    chat_history.append({"role": "user", "content": result})
                    formatted_prompt = llm_tokenizer.apply_chat_template(history=chat_history, add_generation_prompt=True)

                    self.progress_updated.emit(0, "processing...")
                    self.stream_message=""
                    self.stream_sd_prompt_index=None
                    print("running llm!")
                    llm_result = self.llm_pipeline.generate(inputs=formatted_prompt, generation_config=generate_config, streamer=self.llm_streamer)

                    search_string = "SD Prompt:"

                    #sometimes the llm will return 'SD Prompt: None', so filter out that case.
                    if search_string in llm_result and 'None' not in llm_result:
                        # Find the start of the search string
                        start_index = llm_result.find(search_string)
                        # Calculate the start index of the new string (1 character past the ':')
                        prompt = llm_result[start_index + len(search_string):].strip()
                        #print(f"Extracted prompt: '{prompt}'")

                        caption = prompt
                        self.caption_updated.emit(caption)
                        print("calling self.generate_image...")
                        self.progress_updated.emit(0, "illustrating...")
                        
                        self.generate_image(prompt)
                        #self.image_updated.emit(pixmap)  # Emit the QPixmap

                        # this was a meaningful message!
                        meaningful_message_pairs.append(
                        ({"role": "user", "content": result},
                         {"role": "assistant", "content": llm_result},)
                        )

                except Empty:
                    continue  # Queue is empty, just wait
                except Exception as e:
                    print(f"Error in WorkerThread.run: {str(e)}")
                    self.progress_updated.emit(0, f"Error: {str(e)}")
                    continue

            self.progress_updated.emit(0, "idle")
        except Exception as e:
            print(f"Error in WorkerThread.run: {str(e)}")
            self.progress_updated.emit(0, f"Error: {str(e)}")

class ClickableLabel(QLabel):
    clicked = Signal()  # Define a signal to emit on click

    def mousePressEvent(self, event):
        self.clicked.emit()  # Emit the clicked signal
        super().mousePressEvent(event)

class MainWindow(QMainWindow):
    def __init__(self, app_params):
        super().__init__()

        # Main widget and layout
        self.central_widget = QWidget()
        self.setCentralWidget(self.central_widget)
        layout = QGridLayout(self.central_widget)

        self.llm_pipeline = app_params["llm"]
        self.sd_engine = app_params["sd"]
        self.app_params = app_params

        # Image pane
        self.image_label = ClickableLabel("No Image")
        #self.image_label.setFixedSize(1280, 720)
        self.image_label.setFixedSize(1216, 684)
        self.image_label.setStyleSheet("border: 1px solid black;")
        self.image_label.setAlignment(Qt.AlignCenter)
        layout.addWidget(self.image_label, 0, 1)

        # Connect the click signal
        self.display_primary_img = True
        self.image_label.clicked.connect(self.swap_image)

        self.primary_pixmap = None
        self.depth_pixmap = None

        # Caption
        self.caption_label = QLabel("No Caption")
        fantasy_font = QFont("Papyrus", 18, QFont.Bold)
        self.caption_label.setFont(fantasy_font)
        self.caption_label.setAlignment(Qt.AlignCenter)
        self.caption_label.setWordWrap(True)  # Enable word wrapping
        layout.addWidget(self.caption_label, 1, 1)

        # Log widget
        self.log_widget = QTextEdit()
        self.log_widget.setReadOnly(True)
        self.log_widget.setStyleSheet("background-color: #f0f0f0; border: 1px solid gray;")
        layout.addWidget(self.log_widget, 0, 2, 2, 1)
        self.log_widget.hide()  # Initially hidden

        bottom_layout = QVBoxLayout()

        # Bottom pane with buttons and progress bar
        button_layout = QHBoxLayout()
        self.start_button = QPushButton("Start")
        self.start_button.clicked.connect(self.start_thread)
        button_layout.addWidget(self.start_button)

        self.toggle_theme_button = QPushButton("Theme")
        self.toggle_theme_button.clicked.connect(self.toggle_theme)
        button_layout.addWidget(self.toggle_theme_button)

        self.progress_bar = QProgressBar()
        self.progress_bar.setFormat("Idle")
        self.progress_bar.setValue(0)
        button_layout.addWidget(self.progress_bar)

        bottom_layout.addLayout(button_layout)

        # Theme text box, initially hidden
        self.theme_input = QLineEdit()
        self.theme_input.setPlaceholderText("Enter a theme here...")
        self.theme_input.setText("Medieval Fantasty Adventure")
        self.theme_input.setStyleSheet("background-color: white; color: black;")
        self.theme_input.hide()
        bottom_layout.addWidget(self.theme_input)

        layout.addLayout(bottom_layout, 2, 0, 1, 3)

        # Worker threads
        self.speech_thread = None
        self.worker = None

        # Window configuration
        self.setWindowTitle("AI Adventure Experience")
        self.resize(800, 600)

    def start_thread(self):
        if not self.worker or not self.worker.isRunning():
            try:
                self.vad_worker = VADWorker()
                self.vad_worker.start()

                self.whisper_worker = WhisperWorker(self.vad_worker.result_queue, self.app_params["whisper_device"])
                self.whisper_worker.start()

                self.queue = self.whisper_worker.result_queue

                self.worker = WorkerThread(self.queue, self.app_params, self.theme_input.text())
                self.worker.image_updated.connect(self.update_image)
                self.worker.primary_pixmap_updated.connect(self.update_primary_pixmap)
                self.worker.depth_pixmap_updated.connect(self.update_depth_pixmap)
                self.worker.caption_updated.connect(self.update_caption)
                self.worker.progress_updated.connect(self.update_progress)
                self.worker.start()
                self.start_button.setText("Stop")
            except Exception as e:
                print(f"Error starting worker threads: {str(e)}")
                self.caption_label.setText(f"Error: {str(e)}")
                if hasattr(self, 'vad_worker') and self.vad_worker:
                    self.vad_worker.stop()
                if hasattr(self, 'whisper_worker') and self.whisper_worker:
                    self.whisper_worker.stop()
                if hasattr(self, 'worker') and self.worker:
                    self.worker.stop()

        else:
            self.worker.stop()
            self.worker = None

            self.vad_worker.stop()
            self.whisper_worker.stop()

            #self.worker.terminate()
            self.start_button.setText("Start")

            self.queue = None

    def toggle_log(self):
        if self.log_widget.isVisible():
            self.log_widget.hide()
        else:
            self.log_widget.show()

    def toggle_theme(self):
        if self.theme_input.isVisible():
            self.theme_input.hide()
        else:
            self.theme_input.show()

    def update_depth_pixmap(self, pixmap):
        self.depth_pixmap = pixmap

        self.update_image_label()

    def update_primary_pixmap(self, pixmap):
        self.primary_pixmap = pixmap

        self.update_image_label()

    def update_image_label(self):
        if self.display_primary_img and self.primary_pixmap is not None:
            pixmap = self.primary_pixmap
            self.image_label.setPixmap(pixmap.scaled(self.image_label.size()))
        elif not self.display_primary_img and self.depth_pixmap is not None:
            pixmap = self.depth_pixmap
            self.image_label.setPixmap(pixmap.scaled(self.image_label.size()))

    def update_image(self, pixmap):
        print("not doing anything...")
        #pixmap = QPixmap.fromImage(image)
        #self.image_label.setPixmap(pixmap.scaled(self.image_label.size()))

    def swap_image(self):
        self.display_primary_img = (not self.display_primary_img)
        self.update_image_label()

    def update_caption(self, caption):
        self.caption_label.setText(caption)
        #self.log_widget.append(f"Caption updated: {caption}")

    def update_progress(self, value, label):
        self.progress_bar.setValue(value)
        self.progress_bar.setFormat(label)

    def closeEvent(self, event):
        if self.worker and self.worker.isRunning():
            self.vad_worker.stop()
            self.whisper_worker.stop()
            self.worker.stop()  # Gracefully stop the worker thread
            self.worker.wait()  # Wait for the thread to finish

        event.accept()  # Proceed with closing the application

def main():
    # Define the parameters for the application
    app_params = {
        "llm": None,
        "sd": None,
        "whisper_device": None,
        "super_res_compiled_model": None,
        "super_res_upsample_factor": 1,
        "depth_compiled_model": None
    }

    # Create the 'results' folder if it doesn't exist
    Path("results").mkdir(exist_ok=True)

    # Define the devices
    sd_device = "CPU"
    whisper_device = "CPU"
    depth_anything_device = "CPU"

    # Initialize OpenVINO Core
    try:
        core = Core()
        print("OpenVINO Core initialized successfully")
        print("Available devices:", core.available_devices)
    except Exception as e:
        print(f"Error initializing OpenVINO Core: {str(e)}")
        core = Core()  # Use the dummy Core class defined earlier

    # Define a dummy pipeline class for when LLM generation is not available
    class DummyLLMPipeline:
        def generate(self, inputs=None, prompt=None, generation_config=None, streamer=None):
            print(f"Generating text for prompt: {prompt or inputs}")
            if streamer:
                streamer("SD Prompt: A beautiful landscape with mountains and a lake.")
            return "SD Prompt: A beautiful landscape with mountains and a lake."
        
        def get_tokenizer(self):
            class DummyTokenizer:
                def apply_chat_template(self, *args, **kwargs):
                    return ""
            return DummyTokenizer()

    # Try to create a LLM pipeline with the actual model
    try:
        print("Creating a LLM pipeline...")
        
        # Check if the model directory exists
        llm_model_dir = os.path.join("models", "llama-3.2-3b-instruct-openvino")
        if not os.path.exists(llm_model_dir):
            print(f"LLM model directory not found: {llm_model_dir}")
            print("Using dummy LLM pipeline instead")
            app_params["llm"] = DummyLLMPipeline()
        else:
            # Try to create the LLM pipeline
            try:
                config = {
                    "PERFORMANCE_HINT": "LATENCY"
                }
                
                llm_pipe = ov_genai.LLMPipeline(
                    model_path=llm_model_dir,
                    device=sd_device,
                    config=config
                )
                app_params["llm"] = llm_pipe
                print("LLM pipeline created successfully")
            except Exception as e:
                print(f"Error creating LLM pipeline: {str(e)}")
                print("Using dummy LLM pipeline instead")
                app_params["llm"] = DummyLLMPipeline()
    except Exception as e:
        print(f"Error setting up LLM pipeline: {str(e)}")
        print("Using dummy LLM pipeline instead")
        app_params["llm"] = DummyLLMPipeline()

    # Define a dummy pipeline class for when image generation is not available
    class DummyPipeline:
        def generate(self, *args, **kwargs):
            print("Image generation is not available")
            # Create a simple gradient image as a placeholder
            img = np.zeros((432, 768, 3), dtype=np.uint8)
            for i in range(img.shape[0]):
                for j in range(img.shape[1]):
                    img[i, j, 0] = int(255 * i / img.shape[0])  # Red gradient
                    img[i, j, 1] = int(255 * j / img.shape[1])  # Green gradient
                    img[i, j, 2] = 100  # Blue constant
            return type('obj', (object,), {'data': [img]})

    # Try to create a stable diffusion pipeline with the actual model
    try:
        print("Creating a stable diffusion pipeline...")
        
        # Check if the model directory exists
        sd_model_dir = os.path.join("models", "LCM_Dreamshaper_v7", "FP16")
        if not os.path.exists(sd_model_dir):
            print(f"SD model directory not found: {sd_model_dir}")
            print("Using dummy stable diffusion pipeline instead")
            app_params["sd"] = DummyPipeline()
        else:
            # Try to create the stable diffusion pipeline
            try:
                # Try using different class names from openvino_genai
                try:
                    # Try StableDiffusionPipeline first
                    sd_pipe = ov_genai.StableDiffusionPipeline(
                        model=sd_model_dir,
                        device=sd_device
                    )
                    print("Successfully created StableDiffusionPipeline")
                except Exception as e:
                    print(f"Error creating StableDiffusionPipeline: {str(e)}")
                    try:
                        # Try ImageGenerationPipeline next
                        sd_pipe = ov_genai.ImageGenerationPipeline(
                            model=sd_model_dir,
                            device=sd_device
                        )
                        print("Successfully created ImageGenerationPipeline")
                    except Exception as e:
                        print(f"Error creating ImageGenerationPipeline: {str(e)}")
                        try:
                            # Try Text2ImagePipeline as a last resort
                            sd_pipe = ov_genai.Text2ImagePipeline(
                                model=sd_model_dir,
                                device=sd_device
                            )
                            print("Successfully created Text2ImagePipeline")
                        except Exception as e:
                            print(f"Error creating Text2ImagePipeline: {str(e)}")
                            print("Could not create any image generation pipeline. Using dummy pipeline.")
                            sd_pipe = DummyPipeline()
                
                app_params["sd"] = sd_pipe
                print("Stable diffusion pipeline created successfully")
            except Exception as e:
                print(f"Error creating stable diffusion pipeline: {str(e)}")
                print("Using dummy stable diffusion pipeline instead")
                app_params["sd"] = DummyPipeline()
    except Exception as e:
        print(f"Error setting up stable diffusion pipeline: {str(e)}")
        print("Using dummy stable diffusion pipeline instead")
        app_params["sd"] = DummyPipeline()
    
    # Set up the whisper device
    app_params["whisper_device"] = whisper_device
    
    # Skip loading the super resolution model and use None values
    print("Skipping super resolution model loading")
    app_params["super_res_compiled_model"] = None
    app_params["super_res_upsample_factor"] = 1
    
    # Try to load the depth model
    try:
        print("Initializing Depth Anything v2 model to run on", depth_anything_device)
        OV_DEPTH_ANYTHING_PATH = Path(f"models/depth_anything_v2_vits.xml")
        # Check if the file exists before trying to load it
        if not OV_DEPTH_ANYTHING_PATH.exists():
            print(f"Depth model file not found: {OV_DEPTH_ANYTHING_PATH}")
            app_params["depth_compiled_model"] = None
            print("Will continue without depth map capability")
        else:
            print(f"Loading depth model from: {OV_DEPTH_ANYTHING_PATH}")
            try:
                depth_compiled_model = core.compile_model(OV_DEPTH_ANYTHING_PATH, device_name=depth_anything_device)
                app_params["depth_compiled_model"] = depth_compiled_model
                print("Initializing Depth Anything v2 done...")
            except Exception as e:
                print(f"Error compiling depth model: {str(e)}")
                app_params["depth_compiled_model"] = None
                print("Will continue without depth map capability")
    except Exception as e:
        print(f"Error loading depth model: {str(e)}")
        print("Will continue without depth map capability")
        app_params["depth_compiled_model"] = None
    
    print("Demo is ready!")
    
    # Create and show the main window
    app = QApplication(sys.argv)
    window = MainWindow(app_params)
    window.show()
    
    sys.exit(app.exec())

if __name__ == "__main__":
    main() 